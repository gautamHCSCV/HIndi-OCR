{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1qNVaW75S8juzIFEiWPbKQuwjVg4CZQ2y","authorship_tag":"ABX9TyND5XD5KG/RH/9Rgp72We2O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install onnx\n","!pip install onnxruntime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5U7I-73dsCxy","executionInfo":{"status":"ok","timestamp":1717830728203,"user_tz":-330,"elapsed":20220,"user":{"displayName":"Gautam Kumar","userId":"13184684632697987087"}},"outputId":"a717d2ff-064a-4dfb-dc33-41d619e8e229"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnx\n","  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from onnx) (1.25.2)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (3.20.3)\n","Installing collected packages: onnx\n","Successfully installed onnx-1.16.1\n","Collecting onnxruntime\n","  Downloading onnxruntime-1.18.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting coloredlogs (from onnxruntime)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.25.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.12.1)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n","Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.18.0\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nz6DYLSWqdF9","executionInfo":{"status":"ok","timestamp":1717830825143,"user_tz":-330,"elapsed":6716,"user":{"displayName":"Gautam Kumar","userId":"13184684632697987087"}},"outputId":"ecc7eabd-b8b6-4a0e-d37c-8cb1b862519e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input Shapes: {'input1': [1, 3, 'height', 'width']}\n","Output Shapes: {'output': [1, 'Transposeoutput_dim_1', 'Transposeoutput_dim_2', 2], '281': [1, 32, 'Relu281_dim_2', 'Relu281_dim_3']}\n"]}],"source":["import onnx\n","import onnxruntime as ort\n","\n","def get_model_io_shapes(model_path):\n","    # Load the ONNX model\n","    model = onnx.load(model_path)\n","\n","    # Initialize ONNX Runtime session\n","    session = ort.InferenceSession(model_path)\n","\n","    # Get input and output shapes\n","    input_shapes = {}\n","    for input in session.get_inputs():\n","        input_shapes[input.name] = input.shape\n","\n","    output_shapes = {}\n","    for output in session.get_outputs():\n","        output_shapes[output.name] = output.shape\n","\n","    return input_shapes, output_shapes\n","\n","# Example usage\n","model_path = '/content/drive/MyDrive/OCR-SKU/Easy OCR/EasyOCR ONNX Models/v1/detection_model.onnx'\n","input_shapes, output_shapes = get_model_io_shapes(model_path)\n","\n","print(\"Input Shapes:\", input_shapes)\n","print(\"Output Shapes:\", output_shapes)"]},{"cell_type":"code","source":["model_path = '/content/drive/MyDrive/OCR-SKU/Easy OCR/EasyOCR ONNX Models/v1/13_recognition_model.onnx'\n","input_shapes, output_shapes = get_model_io_shapes(model_path)\n","\n","print(\"Input Shapes:\", input_shapes)\n","print(\"Output Shapes:\", output_shapes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C-GAT5KQrveX","executionInfo":{"status":"ok","timestamp":1717830903578,"user_tz":-330,"elapsed":22215,"user":{"displayName":"Gautam Kumar","userId":"13184684632697987087"}},"outputId":"e09fb829-1bfb-4643-da3c-6c02b2bf3c25"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Shapes: {'input1': [1, 1, 64, 'batch_size_1_1']}\n","Output Shapes: {'output': [1, 'Addoutput_dim_1', 188]}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"CefTc9h8rvga"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_image(image, input_shape):\n","    # Get target height and width\n","    target_height = 64\n","    target_width = 128\n","\n","    height, width, _ = image.shape\n","    aspect_ratio = width / height\n","    new_width = int(target_width)\n","    new_height = int(new_width / aspect_ratio)\n","    if new_height > target_height:\n","        new_height = target_height\n","        new_width = int(new_height * aspect_ratio)\n","    resized_image = cv2.resize(image, (new_width, new_height))\n","\n","    # Ensure the image has 3 color channels\n","    if resized_image.shape[2] != 3:\n","        raise ValueError(\"Input image must have 3 color channels\")\n","\n","    # Pad the image to match the target shape\n","    pad_height = target_height - new_height\n","    pad_width = target_width - new_width\n","    top_pad = pad_height // 2\n","    bottom_pad = pad_height - top_pad\n","    left_pad = pad_width // 2\n","    right_pad = pad_width - left_pad\n","    padded_image = cv2.copyMakeBorder(resized_image, top_pad, bottom_pad, left_pad, right_pad, cv2.BORDER_CONSTANT, value=0)\n","\n","    # Normalize image to range [0, 1]\n","    normalized_image = padded_image.astype(np.float32) / 255.0\n","\n","    # Add batch dimension\n","    batched_image = np.expand_dims(normalized_image, axis=0)\n","\n","    return batched_image"],"metadata":{"id":"rekjYzntxHG0","executionInfo":{"status":"ok","timestamp":1717832531189,"user_tz":-330,"elapsed":2,"user":{"displayName":"Gautam Kumar","userId":"13184684632697987087"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import onnxruntime as ort\n","\n","def load_image(image_path):\n","    # Load image using OpenCV\n","    image = cv2.imread(image_path)\n","    # Convert BGR to RGB\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    return image\n","\n","\n","def perform_detection(image, detection_model_path):\n","    # Load detection model\n","    detection_session = ort.InferenceSession(detection_model_path)\n","\n","    # Print model input shape for debugging\n","    print(\"Model input shape:\", detection_session.get_inputs()[0].shape)\n","\n","    # Preprocess input image\n","    input_name = detection_session.get_inputs()[0].name\n","    input_shape = detection_session.get_inputs()[0].shape\n","    preprocessed_image = preprocess_image(image, input_shape)\n","\n","    # Print preprocessed image shape for debugging\n","    print(\"Preprocessed image shape:\", preprocessed_image.shape)\n","\n","    # Perform inference\n","    detection_results = detection_session.run(None, {input_name: preprocessed_image})\n","\n","    return detection_results\n","\n","def perform_recognition(image, recognition_model_path):\n","    # Load recognition model\n","    recognition_session = ort.InferenceSession(recognition_model_path)\n","\n","    # Preprocess input image\n","    input_name = recognition_session.get_inputs()[0].name\n","    input_shape = recognition_session.get_inputs()[0].shape\n","    preprocessed_image = preprocess_image(image, input_shape)\n","\n","    # Perform inference\n","    recognition_results = recognition_session.run(None, {input_name: preprocessed_image})\n","\n","    return recognition_results\n","\n","# Example usage\n","detection_model_path = '/content/drive/MyDrive/OCR-SKU/Easy OCR/EasyOCR ONNX Models/v1/detection_model.onnx'\n","recognition_model_path = '/content/drive/MyDrive/OCR-SKU/Easy OCR/EasyOCR ONNX Models/v1/13_recognition_model.onnx'\n","image_path = '/content/a.jpg'\n","\n","# Load image\n","image = load_image(image_path)\n","\n","# Perform detection\n","detection_results = perform_detection(image, detection_model_path)\n","print(\"Detection Results:\", detection_results)\n","\n","# Perform recognition\n","recognition_results = perform_recognition(image, recognition_model_path)\n","print(\"Recognition Results:\", recognition_results)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":460},"id":"zbViNUvrrvia","executionInfo":{"status":"error","timestamp":1717832533039,"user_tz":-330,"elapsed":4,"user":{"displayName":"Gautam Kumar","userId":"13184684632697987087"}},"outputId":"9cc1a5b5-3e66-464d-f13d-de0890ddc4e2"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Model input shape: [1, 3, 'height', 'width']\n","Preprocessed image shape: (1, 64, 128, 3)\n"]},{"output_type":"error","ename":"InvalidArgument","evalue":"[ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input1 for the following indices\n index: 1 Got: 64 Expected: 3\n Please fix either the inputs/outputs or the model.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgument\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-3de1bde51b62>\u001b[0m in \u001b[0;36m<cell line: 56>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Perform detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mdetection_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperform_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Detection Results:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetection_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-3de1bde51b62>\u001b[0m in \u001b[0;36mperform_detection\u001b[0;34m(image, detection_model_path)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Perform inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mdetection_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpreprocessed_image\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdetection_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgument\u001b[0m: [ONNXRuntimeError] : 2 : INVALID_ARGUMENT : Got invalid dimensions for input: input1 for the following indices\n index: 1 Got: 64 Expected: 3\n Please fix either the inputs/outputs or the model."]}]},{"cell_type":"code","source":["!pip install easyocr"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_WgdcsxvaYT","executionInfo":{"status":"ok","timestamp":1717832863426,"user_tz":-330,"elapsed":88119,"user":{"displayName":"Gautam Kumar","userId":"13184684632697987087"}},"outputId":"27aed683-ddaa-47cf-ee09-d8b9ad6832d1"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting easyocr\n","  Downloading easyocr-1.7.1-py3-none-any.whl (2.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.3.0+cu121)\n","Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.18.0+cu121)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from easyocr) (4.10.0.82)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.11.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.25.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from easyocr) (9.4.0)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.19.3)\n","Collecting python-bidi (from easyocr)\n","  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.1)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.4)\n","Collecting pyclipper (from easyocr)\n","  Downloading pyclipper-1.3.0.post5-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (908 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ninja (from easyocr)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (4.12.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (1.12.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->easyocr)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->easyocr)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->easyocr)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->easyocr)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->easyocr)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->easyocr)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch->easyocr)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->easyocr)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->easyocr)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch->easyocr)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch->easyocr)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->easyocr) (2.3.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->easyocr)\n","  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi->easyocr) (1.16.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.31.6)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.5.22)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (1.6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->easyocr) (2.1.5)\n","Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->easyocr) (1.3.0)\n","Installing collected packages: pyclipper, ninja, python-bidi, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, easyocr\n","Successfully installed easyocr-1.7.1 ninja-1.11.1.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 pyclipper-1.3.0.post5 python-bidi-0.4.2\n"]}]},{"cell_type":"code","source":["import torch\n","import easyocr\n","\n","reader = easyocr.Reader(['hi'], gpu=True)  # Use 'gpu=True' to utilize GPU if available\n","\n","# Access the detection and recognition models within EasyOCR\n","detector = reader.detector\n","device = 'cpu'\n","\n","batch_size_1 = 500\n","batch_size_2 = 500\n","in_shape=[1, 3, batch_size_1, batch_size_2]\n","dummy_input = torch.rand(in_shape)\n","dummy_input = dummy_input.to(device)\n","\n","torch.onnx.export(\n","    detector,\n","    dummy_input,\n","    \"detectionModel.onnx\",\n","    export_params=True,\n","    opset_version=11,\n","    input_names = ['input'],\n","    output_names = ['output'],\n","    dynamic_axes={'input' : {2 : 'batch_size_1', 3: 'batch_size_2'}},\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jmbgCP4ztoP","executionInfo":{"status":"ok","timestamp":1717833375046,"user_tz":-330,"elapsed":10900,"user":{"displayName":"Gautam Kumar","userId":"13184684632697987087"}},"outputId":"a224e2b7-309e-4b6c-d601-e0cae94d6271"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:easyocr.easyocr:Neither CUDA nor MPS are available - defaulting to CPU. Note: This module is much faster with a GPU.\n"]}]},{"cell_type":"code","source":["import onnxruntime\n","import numpy as np\n","import cv2\n","\n","# Load the ONNX model\n","onnx_model_path = \"detectionModel.onnx\"\n","ort_session = onnxruntime.InferenceSession(onnx_model_path)\n","\n","# Define function to preprocess input image\n","def preprocess_image(image_path):\n","    # Load image using OpenCV\n","    image = cv2.imread(image_path)\n","    # Resize image to match model input shape\n","    image_resized = cv2.resize(image, (500, 500))\n","    # Convert image to float32 and normalize\n","    image_resized = image_resized.astype(np.float32) / 255.0\n","    # Convert image to CHW format (Channel, Height, Width)\n","    image_resized = np.transpose(image_resized, (2, 0, 1))\n","    # Add batch dimension\n","    image_resized = np.expand_dims(image_resized, axis=0)\n","    return image_resized\n","\n","# Define function to perform inference\n","def detect_objects(image_path):\n","    # Preprocess input image\n","    input_data = preprocess_image(image_path)\n","    # Perform inference\n","    outputs = ort_session.run(None, {'input': input_data})\n","    return outputs\n","\n","# Function to post-process detection results\n","def postprocess_detection(detection_output):\n","    # Perform any necessary post-processing on detection output\n","    # For example, you can extract bounding boxes, confidence scores, etc.\n","    # Modify this function based on the output format of your model\n","    return detection_output\n","\n","# Path to input image\n","image_path = \"a.jpg\"\n","\n","# Perform detection inference\n","detection_output = detect_objects(image_path)\n","\n","# Post-process detection output\n","postprocessed_output = postprocess_detection(detection_output)\n","\n","# Print or use post-processed output as required\n","print(len(postprocessed_output), postprocessed_output[0].shape, postprocessed_output[1].shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_gmEhPWP3MAA","executionInfo":{"status":"ok","timestamp":1717833940586,"user_tz":-330,"elapsed":2975,"user":{"displayName":"Gautam Kumar","userId":"13184684632697987087"}},"outputId":"1daaf6da-4933-4c5a-be8e-8daba1cec7da"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["2 (1, 250, 250, 2) (1, 32, 250, 250)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9WyvYo3b3MCF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchvision.transforms as transforms\n","\n","recognizer = reader.recognizer\n","\n","# Define the dimensions of the input image\n","batch_size = 1\n","num_channels = 1\n","image_height = imgH = 64\n","image_width = 128\n","device = 'cpu'\n","\n","# Create dummy input tensors for the image and text inputs\n","dummy_input_image = torch.randn(batch_size, num_channels, image_height, image_width)\n","\n","# Define the maximum length of the text input\n","max_text_length = 10\n","\n","dummy_text_input = torch.LongTensor(max_text_length, batch_size).random_(0, 10)\n","\n","# Convert the input image to grayscale\n","grayscale_transform = transforms.Grayscale(num_output_channels=1)\n","grayscale_input = grayscale_transform(dummy_input_image)\n","\n","input_names = [\"image_input\", \"text_input\"]\n","output_names = [\"output\"]\n","dynamic_axes = {\"image_input\": {0: \"batch_size\"}, \"text_input\": {1: \"batch_size\"}}\n","opset_version = 12\n","\n","torch.onnx.export(recognizer, (grayscale_input, dummy_text_input), \"recog.onnx\",\n","                  input_names=input_names, output_names=output_names,\n","                  dynamic_axes=dynamic_axes, opset_version=opset_version)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"I2Ni3PJiztqM","executionInfo":{"status":"error","timestamp":1717834088979,"user_tz":-330,"elapsed":3646,"user":{"displayName":"Gautam Kumar","userId":"13184684632697987087"}},"outputId":"c93517ca-0a08-4293-b711-0e6b4e6991db"},"execution_count":40,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: torch._C.ScriptObject","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-c5c8bebd9811>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mopset_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m torch.onnx.export(recognizer, (grayscale_input, dummy_text_input), \"recog.onnx\", \n\u001b[0m\u001b[1;32m     31\u001b[0m                   \u001b[0minput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                   dynamic_axes=dynamic_axes, opset_version=opset_version)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \"\"\"\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m     _export(\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1610\u001b[0m             \u001b[0m_validate_dynamic_axes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m             graph, params_dict, torch_out = _model_to_graph(\n\u001b[0m\u001b[1;32m   1613\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1614\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pre_trace_quant_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m     \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_jit_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m     \u001b[0mparams_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_named_param_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    992\u001b[0m             \u001b[0margs_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0mparam_count_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_param_count_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             \u001b[0min_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             graph = _C._propagate_and_assign_input_shapes(\n\u001b[1;32m    996\u001b[0m                 \u001b[0mmethod_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_count_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Only tuples, lists and Variables are supported as JIT inputs/outputs. Dictionaries and strings are also accepted, but their usage is not recommended. Here, received an input of unsupported type: torch._C.ScriptObject"]}]},{"cell_type":"code","source":[],"metadata":{"id":"2mPVRXRD1wCl"},"execution_count":null,"outputs":[]}]}